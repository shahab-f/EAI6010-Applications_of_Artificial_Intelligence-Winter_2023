{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahab-f/EAI6010-Applications_of_Artificial_Intelligence-Winter_2023/blob/Textual-Data-Analysis-with-NLTK-in-Python/EAI6010_ShahabaddinFeghahati_Week3_Winter_2023_rev_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EAI6010 - Module 3: NLP AI Applications"
      ],
      "metadata": {
        "id": "IdlNHZrQ9Sf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This report presents code examples for analyzing textual data using the Natural Language Toolkit (NLTK) library in Python. The code covers two main topics: syntax analysis for modal verbs and word frequency analysis for a particular speech.\n",
        "\n",
        "The first section of the code demonstrates how to install and import the NLTK library, download a corpus, and use it to count the frequency and relative frequency of modal verbs. The second section focuses on analyzing the frequency of long words in a particular speech, identifying the ten most common long words, and finding synonyms for these words using the WordNet database. The report includes code examples for each step of the analysis, along with explanations of the syntax and methodology used."
      ],
      "metadata": {
        "id": "JImqoIgD9twE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Body"
      ],
      "metadata": {
        "id": "EDqRVetE9vBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 1A. Download and install the Gutenberg corpus tool to your Jupyter Notebook. You can install the NLTK package and use Gutenberg corpus.\n",
        "\n",
        "#### Answer 1A:"
      ],
      "metadata": {
        "id": "dk5OSh0GT95L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1A & 1B Syntax Guide\n",
        "\n",
        "# Install NLTK package\n",
        "#!pip install nltk\n",
        "\n",
        "# Import NLTK\n",
        "import nltk\n"
      ],
      "metadata": {
        "id": "zl_j5hxL9bUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 1B. Download the Gutenberg corpus tool in the NLTK package.\n",
        "\n",
        "#### Answer 1B:"
      ],
      "metadata": {
        "id": "OeaGv17cqa3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1B Syntax Guide\n",
        "\n",
        "# Download Gutenberg corpus\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5bkVLUnqc3h",
        "outputId": "8959a385-db58-41bd-e478-1729c9e6c6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 1C. Use the texts in the corpus.\n",
        "\n",
        "#### Question 1D. Create a table displaying relative frequencies with which “modals” (can, could, may, might, will, would, and should) appear in all texts provided in the corpus.\n",
        "\n",
        "#### Answer 1C & 1D:"
      ],
      "metadata": {
        "id": "HVP2uQgxT1MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1C & 1D Syntax Guide\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "import pandas as pd\n",
        "\n",
        "# Get all modal verbs\n",
        "modals = ['can', 'could', 'may', 'might', 'will', 'would', 'should']\n",
        "\n",
        "# Count the frequency of each modal verb in the corpus\n",
        "freqs = nltk.FreqDist([w.lower() for w in gutenberg.words() if w.lower() in modals])\n",
        "\n",
        "# Calculate the total number of words in the corpus\n",
        "num_words = len(gutenberg.words())\n",
        "\n",
        "# Create a pandas DataFrame to store the frequency and relative frequency of each modal verb\n",
        "df = pd.DataFrame({'Frequency': [freqs[modal] for modal in modals],\n",
        "                   'Relative Frequency': [freqs[modal] / num_words for modal in modals]},\n",
        "                  index=modals)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuaUiO7SA7Bv",
        "outputId": "19fa0034-5798-42be-b51b-744dc98ac796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Frequency  Relative Frequency\n",
            "can          2327            0.000888\n",
            "could        3594            0.001371\n",
            "may          2549            0.000972\n",
            "might        1963            0.000749\n",
            "will         7368            0.002810\n",
            "would        4046            0.001543\n",
            "should       2550            0.000973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data provided shows the relative frequencies of modal verbs in the Gutenberg corpus. The modal verb \"will\" is the most frequent, with a relative frequency of 0.002810, followed by \"would\" with 0.001543, and \"might\" with 0.000749. The relative frequencies of the seven modal verbs range from 0.000749 to 0.002810, indicating that modal verbs are relatively rare compared to other words in the corpus. However, the total relative frequency of all modal verbs is 0.009866, indicating that they still occur frequently enough to be worthy of analysis."
      ],
      "metadata": {
        "id": "67e3wxB7DJ9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 1E. For two modals with the largest span of relative (to the total number of modals) frequencies (most used minus least used), select a text which uses it the most and the text that uses it the least. Compare usage in both texts by examining the relative frequencies of those modals in the two texts. Try to explain why those words are used differently in the two texts.\n",
        "\n",
        "#### Answer 1E:"
      ],
      "metadata": {
        "id": "mA8737lsUnAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1E Syntax Guide\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Find the two modals with the largest span of relative frequencies\n",
        "sorted_modals = sorted(modals, key=lambda x: freqs[x] / num_words, reverse=True)\n",
        "modal1, modal2 = sorted_modals[:2]\n",
        "\n",
        "# Find the text that uses each modal the most and the least\n",
        "most_text1 = ''\n",
        "most_text2 = ''\n",
        "least_text1 = ''\n",
        "least_text2 = ''\n",
        "most_freq1 = 0\n",
        "most_freq2 = 0\n",
        "least_freq1 = float('inf')\n",
        "least_freq2 = float('inf')\n",
        "for fileid in gutenberg.fileids():\n",
        "    words = gutenberg.words(fileid)\n",
        "    freq1 = words.count(modal1) / len(words)\n",
        "    freq2 = words.count(modal2) / len(words)\n",
        "    if freq1 > most_freq1:\n",
        "        most_text1 = fileid\n",
        "        most_freq1 = freq1\n",
        "    if freq2 > most_freq2:\n",
        "        most_text2 = fileid\n",
        "        most_freq2 = freq2\n",
        "    if freq1 < least_freq1:\n",
        "        least_text1 = fileid\n",
        "        least_freq1 = freq1\n",
        "    if freq2 < least_freq2:\n",
        "        least_text2 = fileid\n",
        "        least_freq2 = freq2\n",
        "\n",
        "# Create a table to display the results\n",
        "table = PrettyTable()\n",
        "table.field_names = ['', modal1, modal2]\n",
        "table.add_row(['Most used text', most_text1, most_text2])\n",
        "table.add_row(['Relative frequency', '{:.6f}'.format(most_freq1), '{:.6f}'.format(most_freq2)])\n",
        "table.add_row(['Least used text', least_text1, least_text2])\n",
        "table.add_row(['Relative frequency', '{:.6f}'.format(least_freq1), '{:.6f}'.format(least_freq2)])\n",
        "\n",
        "# Print the table\n",
        "print(table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGoi16hBK2K6",
        "outputId": "927cf915-baf5-4250-bc0e-5287b446b24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------------+-----------------+\n",
            "|                    |          will          |      would      |\n",
            "+--------------------+------------------------+-----------------+\n",
            "|   Most used text   | shakespeare-caesar.txt | austen-emma.txt |\n",
            "| Relative frequency |        0.004994        |     0.004235    |\n",
            "|  Least used text   |    blake-poems.txt     | blake-poems.txt |\n",
            "| Relative frequency |        0.000359        |     0.000359    |\n",
            "+--------------------+------------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variation in the use of the words above can be explained by several factors, including the time period, genre, and context of the texts. Shakespeare, Blake, and Austen, were from different genres, wrote in distinct styles and forms that reflected their historical and literary contexts (WILLIAM BLAKE, n.d.). In general, the diverse contexts and unique styles and themes of the authors may have influenced the different patterns of using modal verbs in their respective texts."
      ],
      "metadata": {
        "id": "eS2xgUh5Rp9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2A. Downloaded the inaugural corpus NLTK.\n",
        "\n",
        "#### Answer 2A:"
      ],
      "metadata": {
        "id": "YWSi4Qt1wNLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## @title 2A Syntax Guide\n",
        "\n",
        "# Imported the inaugural corpus from NLTK\n",
        "nltk.download(\"inaugural\")\n",
        "from nltk.corpus import inaugural"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCM832gAu1Kp",
        "outputId": "125e4aff-434f-402d-9991-2b03c8b9a689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2B. Choose Kennedy's speech.\n",
        "\n",
        "#### Answer 2B:"
      ],
      "metadata": {
        "id": "ih_UvT1rwVXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2B Syntax Guide\n",
        "\n",
        "# Get the text of Kennedy's speech\n",
        "nltk.corpus.inaugural.words(\"1961-Kennedy.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMo8mkmdwk0d",
        "outputId": "b01d54a7-53f0-4e8e-8998-fb39bedddd2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Vice', 'President', 'Johnson', ',', 'Mr', '.', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2C. Identify the 10 most frequently used long words (words longer than 7 characters).\n",
        "\n",
        "#### Answer 2C:"
      ],
      "metadata": {
        "id": "zi2Ytl9pw_k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2C Syntax Guide\n",
        "\n",
        "# Load Kennedy's speech\n",
        "speech = nltk.corpus.inaugural.words('1961-Kennedy.txt')\n",
        "\n",
        "# Create a list of long words (words longer than 7 characters)\n",
        "long_words = [word.lower() for word in speech if len(word) > 7]\n",
        "\n",
        "# Use FreqDist to count the frequency of each word\n",
        "word_freq = nltk.FreqDist(long_words)\n",
        "\n",
        "# Get the 10 most common words and print them\n",
        "for word, frequency in word_freq.most_common(10):\n",
        "    print(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dws41oB4OK70",
        "outputId": "bfc49d12-cfdb-4244-8bb3-2431e2d731d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "citizens\n",
            "president\n",
            "americans\n",
            "generation\n",
            "forebears\n",
            "revolution\n",
            "committed\n",
            "powerful\n",
            "supporting\n",
            "themselves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the list of the 10 most commonly used long words in Kennedy's speech, we can get some insights into the themes and ideas that Kennedy emphasized in his speech. The list of the 10 most commonly used long words in Kennedy's speech suggests a focus on the power of the presidency, national identity and unity, honoring the past and looking towards the future, and the importance of collective action (John F. Kennedy’s Inaugural Address: An Analysis, n.d.)."
      ],
      "metadata": {
        "id": "dORCRaIVx3AA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2D. Which one of those 10 words has the largest number of synonyms? Use WordNet as a helper:\n",
        "\n",
        "#### Answer 2D:"
      ],
      "metadata": {
        "id": "TcbnBnafx5Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2D Syntax Guide\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# initialize variables for the word with the most synonyms and its number of synonyms\n",
        "max_synonyms_word = ''\n",
        "max_synonyms_count = 0\n",
        "\n",
        "# iterate over the top 10 words\n",
        "for word, count in top_long_words:\n",
        "    # get the synonyms for the word\n",
        "    synonyms = wn.synsets(word)\n",
        "    synonyms_count = len(synonyms)\n",
        "    # update the variables if the word has more synonyms than the previous max\n",
        "    if synonyms_count > max_synonyms_count:\n",
        "        max_synonyms_word = word\n",
        "        max_synonyms_count = synonyms_count\n",
        "\n",
        "# print the result\n",
        "print(\"The word with the most synonyms among the top 10 most frequently used long words is '{}' with {} synonyms.\".format(max_synonyms_word, max_synonyms_count))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX4Zxb1gdXNP",
        "outputId": "7f59130d-4733-4895-e49f-bbc1142ff8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word with the most synonyms among the top 10 most frequently used long words is 'supporting' with 14 synonyms.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One possible insight that can be gleaned from the aforementioned results is that In his speech, Kennedy frequently used the word supporting to convey solidarity and cooperation with various groups and causes, and its wide range of synonyms enabled him to express different aspects of support, such as giving financial aid, providing emotional encouragement, strengthening something physically, and agreeing with or approving something. By using this word, Kennedy appealed to different audiences and contexts and emphasized his commitment to various issues and values."
      ],
      "metadata": {
        "id": "qoQcQhZfHlct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2E. List all synonyms for the 10 most frequently used words. Which one of those 10 words has the largest number of hyponyms?\n",
        "\n",
        "#### Answer 2E:"
      ],
      "metadata": {
        "id": "1VDOFNm1fP4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2E Syntax Guide\n",
        "\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Choose Kennedy's speech\n",
        "#speech = inaugural.words('1961-Kennedy.txt')\n",
        "\n",
        "# Identify the 10 most frequently used long words\n",
        "long_words = [word.lower() for word in speech if len(word) > 7]\n",
        "top_long_words = Counter(long_words).most_common(10)\n",
        "\n",
        "# Find all synonyms for the top 10 long words using WordNet\n",
        "synonyms = {}\n",
        "for word, count in top_long_words:\n",
        "    if wn.synsets(word, pos=wn.NOUN):\n",
        "        pos = wn.NOUN\n",
        "    elif wn.synsets(word, pos=wn.ADJ):\n",
        "        pos = wn.ADJ\n",
        "    elif wn.synsets(word, pos=wn.ADV):\n",
        "        pos = wn.ADV\n",
        "    else:\n",
        "        continue\n",
        "    synsets = wn.synsets(word, pos=pos)\n",
        "    synonyms[word] = set()\n",
        "    for synset in synsets:\n",
        "        for lemma in synset.lemmas():\n",
        "            synonyms[word].add(lemma.name())\n",
        "    synonyms[word].discard(word)  # Remove the original word from the synonyms\n",
        "\n",
        "# Find the long word with the most hyponyms\n",
        "most_hyponyms = 0\n",
        "most_hyponyms_word = None\n",
        "for word, count in top_long_words:\n",
        "    try:\n",
        "        if wn.synsets(word, pos=wn.NOUN):\n",
        "            pos = wn.NOUN\n",
        "        elif wn.synsets(word, pos=wn.ADJ):\n",
        "            pos = wn.ADJ\n",
        "        elif wn.synsets(word, pos=wn.ADV):\n",
        "            pos = wn.ADV\n",
        "        else:\n",
        "            continue\n",
        "        hyponyms = wn.synset(word + '.' + pos + '.01').hyponyms()\n",
        "        if len(hyponyms) > most_hyponyms:\n",
        "            most_hyponyms = len(hyponyms)\n",
        "            most_hyponyms_word = word\n",
        "    except:\n",
        "        # If the word doesn't have a sense in WordNet, skip it\n",
        "        continue\n",
        "\n",
        "# Print the synonyms for the top 10 long words and the word with the most hyponyms\n",
        "print(\"Synonyms for the top 10 long words:\")\n",
        "for word, count in top_long_words:\n",
        "    if word in synonyms:\n",
        "        print(word, synonyms[word])\n",
        "    else:\n",
        "        print(word, set())\n",
        "print(\"The long word with the most hyponyms is:\", most_hyponyms_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd3196b-ff1b-4256-b259-4f5e98cad23b",
        "id": "9hFHCjhob5OD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms for the top 10 long words:\n",
            "citizens {'citizen'}\n",
            "president {'United_States_President', 'chair', 'prexy', 'Chief_Executive', 'chairwoman', 'chairman', 'President_of_the_United_States', 'chairperson', 'President'}\n",
            "americans {'American', 'American_English', 'American_language'}\n",
            "generation {'propagation', 'coevals', 'contemporaries', 'genesis', 'multiplication'}\n",
            "forebears {'forebear', 'forbear'}\n",
            "revolution {'rotation', 'gyration'}\n",
            "committed {'attached'}\n",
            "powerful {'knock-down', 'brawny', 'sinewy', 'muscular', 'potent', 'herculean', 'hefty'}\n",
            "supporting {'support'}\n",
            "themselves set()\n",
            "The long word with the most hyponyms is: generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the code for question 2E, it is essential to understand how WordNet organizes different parts of speech into synsets. While attempting to find synonyms for 'committed', 'powerful', and 'themselves', the code used the pos=wordnet.NOUN parameter in the synsets function. However, since these words do not have a noun sense in WordNet, the code did not provide any synonyms for them (WordNet Documentation, n.d.).\n",
        "\n",
        "To address this issue, we need to look for their synsets using the appropriate part of speech. For instance, 'committed' and 'powerful' are adjectives, and 'themselves' is a pronoun. Therefore, we can use pos=wordnet.ADJ for adjectives and pos=wordnet.ADV for adverbs to find their synonyms in WordNet (WordNet Documentation, n.d.).\n",
        "\n",
        "It's worth noting that the word \"themselves\" is a reflexive pronoun, and it's unlikely to have synonyms. Hence, it's crucial to revise the code accordingly to include the appropriate part of speech parameter to retrieve synonyms from WordNet."
      ],
      "metadata": {
        "id": "0uH5M9zvXHzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 2F.List all hyponyms of the 10 most frequently used words.\n",
        "\n",
        "#### Answer 2F:"
      ],
      "metadata": {
        "id": "7cUcqig6ZDgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2F Syntax Guide\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import inaugural\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "from collections import Counter\n",
        "\n",
        "# download the punkt tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# choose Kennedy's speech\n",
        "kennedy_speech = inaugural.raw('1961-Kennedy.txt')\n",
        "\n",
        "# identify the 10 most frequently used long words\n",
        "words = word_tokenize(kennedy_speech)\n",
        "long_words = [word for word in words if len(word) > 7]\n",
        "most_common = Counter(long_words).most_common(10)\n",
        "print('10 most frequently used long words:\\n')\n",
        "for word in most_common:\n",
        "    print(f\"{word[0]}: {word[1]}\")\n",
        "    print('----------------------------------------')\n",
        "\n",
        "# list all hyponyms for the 10 most frequently used words\n",
        "hyponyms = {}\n",
        "for word in most_common:\n",
        "    syns = wn.synsets(word[0])\n",
        "    if syns:\n",
        "        hyponyms[word[0]] = [hypo.name() for s in syns for hypo in s.hyponyms()]\n",
        "\n",
        "print('\\nHyponyms of the 10 most frequently used long words:\\n')\n",
        "for word in hyponyms:\n",
        "    print(f\"{word}:\")\n",
        "    print(*hyponyms[word], sep='\\n')\n",
        "    print('----------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V02Xomunm-6w",
        "outputId": "3039d334-637f-49e7-c048-1b28bfb7a1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 most frequently used long words:\n",
            "\n",
            "citizens: 5\n",
            "----------------------------------------\n",
            "President: 4\n",
            "----------------------------------------\n",
            "Americans: 4\n",
            "----------------------------------------\n",
            "generation: 3\n",
            "----------------------------------------\n",
            "forebears: 2\n",
            "----------------------------------------\n",
            "revolution: 2\n",
            "----------------------------------------\n",
            "committed: 2\n",
            "----------------------------------------\n",
            "powerful: 2\n",
            "----------------------------------------\n",
            "supporting: 2\n",
            "----------------------------------------\n",
            "themselves: 2\n",
            "----------------------------------------\n",
            "\n",
            "Hyponyms of the 10 most frequently used long words:\n",
            "\n",
            "citizens:\n",
            "active_citizen.n.01\n",
            "civilian.n.01\n",
            "freeman.n.01\n",
            "private_citizen.n.01\n",
            "repatriate.n.01\n",
            "thane.n.02\n",
            "voter.n.01\n",
            "----------------------------------------\n",
            "President:\n",
            "ex-president.n.01\n",
            "kalon_tripa.n.01\n",
            "vice_chairman.n.01\n",
            "----------------------------------------\n",
            "Americans:\n",
            "african-american.n.01\n",
            "alabaman.n.01\n",
            "alaskan.n.01\n",
            "anglo-american.n.01\n",
            "appalachian.n.01\n",
            "arizonan.n.01\n",
            "arkansan.n.01\n",
            "asian_american.n.01\n",
            "bay_stater.n.01\n",
            "bostonian.n.01\n",
            "californian.n.01\n",
            "carolinian.n.01\n",
            "coloradan.n.01\n",
            "connecticuter.n.01\n",
            "creole.n.02\n",
            "delawarean.n.01\n",
            "floridian.n.01\n",
            "franco-american.n.01\n",
            "georgian.n.01\n",
            "german_american.n.01\n",
            "hawaiian.n.02\n",
            "idahoan.n.01\n",
            "illinoisan.n.01\n",
            "indianan.n.01\n",
            "iowan.n.01\n",
            "kansan.n.01\n",
            "kentuckian.n.01\n",
            "louisianan.n.01\n",
            "mainer.n.01\n",
            "marylander.n.01\n",
            "michigander.n.01\n",
            "minnesotan.n.01\n",
            "mississippian.n.02\n",
            "missourian.n.01\n",
            "montanan.n.01\n",
            "nebraskan.n.01\n",
            "nevadan.n.01\n",
            "new_englander.n.01\n",
            "new_hampshirite.n.01\n",
            "new_jerseyan.n.01\n",
            "new_mexican.n.01\n",
            "new_yorker.n.01\n",
            "nisei.n.01\n",
            "north_carolinian.n.01\n",
            "north_dakotan.n.01\n",
            "ohioan.n.01\n",
            "oklahoman.n.01\n",
            "oregonian.n.01\n",
            "pennsylvanian.n.02\n",
            "puerto_rican.n.01\n",
            "rhode_islander.n.01\n",
            "south_carolinian.n.01\n",
            "south_dakotan.n.01\n",
            "southerner.n.01\n",
            "spanish_american.n.01\n",
            "tennessean.n.01\n",
            "texan.n.01\n",
            "tory.n.01\n",
            "utahan.n.01\n",
            "vermonter.n.01\n",
            "virginian.n.01\n",
            "washingtonian.n.01\n",
            "washingtonian.n.02\n",
            "west_virginian.n.01\n",
            "wisconsinite.n.01\n",
            "wyomingite.n.01\n",
            "yankee.n.01\n",
            "yankee.n.03\n",
            "african_american_vernacular_english.n.01\n",
            "creole.n.01\n",
            "latin_american.n.01\n",
            "mesoamerican.n.01\n",
            "north_american.n.01\n",
            "south_american.n.01\n",
            "west_indian.n.01\n",
            "----------------------------------------\n",
            "generation:\n",
            "peer_group.n.01\n",
            "youth_culture.n.01\n",
            "baby_boom.n.01\n",
            "generation_x.n.01\n",
            "posterity.n.02\n",
            "biogenesis.n.02\n",
            "----------------------------------------\n",
            "forebears:\n",
            "grandparent.n.01\n",
            "great_grandparent.n.01\n",
            "----------------------------------------\n",
            "revolution:\n",
            "cultural_revolution.n.01\n",
            "green_revolution.n.01\n",
            "counterrevolution.n.01\n",
            "axial_rotation.n.01\n",
            "dextrorotation.n.01\n",
            "levorotation.n.01\n",
            "orbital_rotation.n.01\n",
            "spin.n.01\n",
            "----------------------------------------\n",
            "committed:\n",
            "make.v.24\n",
            "recommit.v.01\n",
            "apply.v.10\n",
            "rededicate.v.01\n",
            "vow.v.02\n",
            "hospitalize.v.01\n",
            "commend.v.03\n",
            "consign.v.02\n",
            "obligate.v.02\n",
            "recommit.v.02\n",
            "buy_into.v.01\n",
            "fund.v.04\n",
            "roll_over.v.03\n",
            "shelter.v.02\n",
            "speculate.v.04\n",
            "tie_up.v.02\n",
            "----------------------------------------\n",
            "powerful:\n",
            "\n",
            "----------------------------------------\n",
            "supporting:\n",
            "shoring.n.02\n",
            "suspension.n.06\n",
            "help.v.01\n",
            "patronize.v.02\n",
            "promote.v.01\n",
            "second.v.01\n",
            "sponsor.v.02\n",
            "undergird.v.01\n",
            "fund.v.06\n",
            "provide.v.06\n",
            "see_through.v.01\n",
            "sponsor.v.01\n",
            "subsidize.v.01\n",
            "champion.v.01\n",
            "guarantee.v.04\n",
            "block.v.11\n",
            "brace.v.03\n",
            "bracket.v.01\n",
            "buoy.v.02\n",
            "carry.v.05\n",
            "chock.v.02\n",
            "pole.v.02\n",
            "prop_up.v.01\n",
            "scaffold.v.01\n",
            "truss.v.03\n",
            "underpin.v.01\n",
            "back.v.09\n",
            "document.v.02\n",
            "prove.v.02\n",
            "validate.v.02\n",
            "verify.v.01\n",
            "vouch.v.04\n",
            "apologize.v.02\n",
            "stand_up.v.05\n",
            "uphold.v.02\n",
            "accept.v.07\n",
            "bear_up.v.01\n",
            "pay.v.09\n",
            "sit_out.v.02\n",
            "stand_for.v.04\n",
            "take_a_joke.v.01\n",
            "take_lying_down.v.01\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights from the output suggest that the speech is likely about patriotism, directed towards young people, calling for change, and encouraging action and support. The tone is motivational and emphasizes the power of heritage and determination."
      ],
      "metadata": {
        "id": "7U4vjORcisRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In conclusion, this report demonstrates the use of the Natural Language Toolkit (NLTK) in Python to analyze textual data. We began by installing and importing the NLTK package, and then downloaded and analyzed the Gutenberg corpus to count the frequency of modal verbs. We also analyzed President Kennedy's inaugural speech by identifying the most frequently used long words and their synonyms using WordNet.\n",
        "\n",
        "Based on the results of our analysis, we suggest that those conducting similar analyses consider exploring other corpora and texts, as well as conducting more sophisticated analyses, such as sentiment analysis or topic modeling."
      ],
      "metadata": {
        "id": "LfTGN1xjejJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "*John F. Kennedy’s Inaugural Address: An Analysis.* (n.d.). EDUZAURUS. Retrieved March 9, 2023, from https://eduzaurus.com/free-essay-samples/john-f-kennedys-inaugural-address-an-analysis/\n",
        "\n",
        "*Will vs. Would: What’s The Difference? (2022, September 1).* Thesaurus. https://www.thesaurus.com/e/grammar/will-vs-would/\n",
        "\n",
        "*WILLIAM BLAKE.* (n.d.). Litpriest. Retrieved March 9, 2023, from https://litpriest.com/authors/william-blake/\n",
        "\n",
        "*WordNet Documentation.* (n.d.). wordnet.princeton.edu. Retrieved March 10, 2023, from https://wordnet.princeton.edu/documentation"
      ],
      "metadata": {
        "id": "Iaj6FnsMRZGx"
      }
    }
  ]
}